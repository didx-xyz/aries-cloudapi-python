fullnameOverride: multitenant-agent

replicaCount: 1

podAnnotations:
  sidecar.istio.io/proxyCPU: 10m
  ad.datadoghq.com/multitenant-agent.logs: '[{"source": "python", "service": "multitenant-agent", "auto_multi_line_detection": true}]'

image:
  name: d-cloud/multitenant-agent
  tag: master
  pullPolicy: Always

podLabels:
  admission.datadoghq.com/enabled: "false"

command:
  - aca-py
  - start
  - --inbound-transport
  - http
  - 0.0.0.0
  - 3020
  - --admin
  - 0.0.0.0
  - 3021
  - --plugin
  - acapy_wallet_groups_plugin
  - --auto-promote-author-did
  - --plugin
  - nats_events.v1_0.nats_queue.events
  - --plugin-config-value
  - nats_queue.connection.connection_url="$(NATS_SERVER)"

lifecycle:
  preStop:
    exec:
      command:
        - /bin/sh
        - -c
        - sleep 10

ingressDomain: acapy-cloud.dev.didxtech.com
ingress:
  internal:
    enabled: true
    className: nginx-internal
    rules:
      - host: multitenant-agent-didcomm-{{ .Values.ingressDomain }}
        paths:
          - path: /
            port: 3020
  internal-e2e:
    enabled: true
    className: nginx-internal
    rules:
      - host: multitenant-agent-{{ .Values.ingressDomain }}
        paths:
          - path: /
            port: 3021

service:
  # if set, will run Pods on Node Network
  hostNetwork: false
  port: 3021
  containerPort: 3021
  appProtocol: tcp

addPorts:
  - port: 3020
    containerPort: 3020
    protocol: TCP

livenessProbe:
  httpGet:
    path: /status/live
    port: "{{ trunc 15 .Release.Name }}"
  initialDelaySeconds: 300
  timeoutSeconds: 30
readinessProbe:
  httpGet:
    path: /status/ready
    port: "{{ trunc 15 .Release.Name }}"
  initialDelaySeconds: 5
  timeoutSeconds: 30

# resources:
#   requests:
#     cpu: 100m
#     memory: 256Mi
#   limits:
#     cpu: 500m
#     memory: 512Mi

initContainers:
  - name: wait-for-ledger-browser
    image: curlimages/curl
    command:
      - sh
      - -c
      - |
        until curl -s http://ledger-browser:8000/status -o /dev/null; do
          echo "waiting for ledger-browser to be healthy"
          sleep 2
        done

persistence:
  enabled: true
  mountPath: /home/aries/.indy_client
  capacity: 1Gi
  storageClassName: efs
  accessMode: ReadWriteOnce

autoscaling:
  enabled: false

# Sensitive environment variables are sourced from k8s secrets:
# - generated with secretData, or
# - pre-populated with external tooling
# TODO: Helm secret logic to create new secret if not exist
secretData:
  ACAPY_WALLET_KEY: verySecretMultitenantWalletKey
  ACAPY_ADMIN_API_KEY: adminApiKey
  ACAPY_MULTITENANT_JWT_SECRET: verySecretMultitenantJwtSecret
  ACAPY_GOVERNANCE_AGENT_API_KEY: adminApiKey
  ACAPY_LABEL: Multitenant
  ACAPY_WALLET_NAME: multitenant
  ACAPY_MULTITENANCY_CONFIGURATION: '{ "wallet_type":"single-wallet-askar", "wallet_name":"multitenant" }'

  ACAPY_WALLET_STORAGE_CONFIG: '{ "max_connections":10, "min_idle_count":10, "url":"cloudapi-postgresql:5432" }'
  ACAPY_WALLET_STORAGE_CREDS: '{ "account":"multitenant", "admin_account":"multitenant", "admin_password":"multitenant", "password":"multitenant" }'
  WALLET_DB_ADMIN_PASS: multitenant
  WALLET_DB_ADMIN_USER: multitenant
  WALLET_DB_HOST: cloudapi-postgresql
  WALLET_DB_PASS: multitenant
  WALLET_DB_PORT: 5432
  WALLET_DB_USER: multitenant

env:
  ACAPY_LOG_LEVEL: info
  # NATS related
  NATS_CREDS_FILE: "" # NATS in Local dev has no auth
  NATS_SERVER: nats://nats:4222
  NATS_SUBJECT: cloudapi.aries.events
  NATS_STREAM: cloudapi_aries_events
  # for aca-py
  ADMIN_URL: http://multitenant-agent:3021
  ACAPY_OUTBOUND_TRANSPORT: http
  ACAPY_ADMIN: "[0.0.0.0,3021]"
  ACAPY_ENDPOINT: http://multitenant-agent:3020
  # Tails server
  ACAPY_TAILS_SERVER_BASE_URL: http://tails-server:6543

  ACAPY_WALLET_TYPE: askar
  ACAPY_WALLET_STORAGE_TYPE: postgres_storage
  ACAPY_AUTO_PROVISION: true
  # Ledger
  ACAPY_GENESIS_URL: http://ledger-browser:8000/genesis

  # Multi-tenant Configuration
  ACAPY_MULTITENANT: true
  ACAPY_MULTITENANT_ADMIN: false
  ACAPY_PUBLIC_INVITES: true
  # ## DO NOT CHANGE VARIABLES BELOW
  # ## Unless you know exactly what you are doing
  # ## Changes will probably break CloudAPI
  # Optional Helper Configurations - See https://github.com/hyperledger/aries-cloudagent-python/blob/main/aries_cloudagent/config/argparse.py
  ACAPY_AUTO_ACCEPT_INVITES: true
  ACAPY_AUTO_ACCEPT_REQUESTS: true
  ACAPY_AUTO_PING_CONNECTION: true
  ACAPY_AUTO_RESPOND_MESSAGES: false
  ACAPY_AUTO_RESPOND_CREDENTIAL_PROPOSAL: false
  ACAPY_AUTO_RESPOND_CREDENTIAL_OFFER: false
  ACAPY_AUTO_RESPOND_CREDENTIAL_REQUEST: false
  ACAPY_AUTO_RESPOND_PRESENTATION_PROPOSAL: false
  ACAPY_AUTO_RESPOND_PRESENTATION_REQUEST: false
  ACAPY_AUTO_STORE_CREDENTIAL: true
  ACAPY_AUTO_VERIFY_PRESENTATION: true
  ACAPY_PRESERVE_EXCHANGE_RECORDS: false
  ACAPY_CREATE_REVOCATION_TRANSACTIONS: true
  # Endorser
  ACAPY_ENDORSER_ROLE: author
  ACAPY_AUTO_REQUEST_ENDORSEMENT: true
  ACAPY_AUTO_WRITE_TRANSACTIONS: true
  ACAPY_ENDORSER_ALIAS: endorser

  ACAPY_REQUESTS_THROUGH_PUBLIC_DID: true
  ACAPY_EMIT_NEW_DIDCOMM_PREFIX: true
  ACAPY_EMIT_NEW_DIDCOMM_MIME_TYPE: true

  # ## From mt-agent-env secret
  # ACAPY_MULTITENANCY_CONFIGURATION: '{"wallet_type":"askar-profile","wallet_name":"xxx"}'

  # ACAPY_LOG_CONFIG: /home/aries/logging_config.yaml

podAntiAffinityPreset: soft
nodeAffinityPreset:
  type: soft
  key: node.kubernetes.io/lifecycle
  values:
    - spot

configFiles:
  logging_config.yml:
    path: /home/aries/logging_config.yaml
    content: |-
      version: 1
      disable_existing_loggers: False
      formatters:
        json_formatter:
          (): pythonjsonlogger.jsonlogger.JsonFormatter
          format: '%(asctime)s %(name)s %(wallet_id)s %(levelname)s %(pathname)s:%(lineno)d %(message)s'
      handlers:
        stream_handler:
          class: logging.StreamHandler
          level: DEBUG
          formatter: json_formatter
          stream: ext://sys.stderr
        timed_file_handler:
          class: logging.handlers.TimedRotatingFileHandler
          level: DEBUG
          formatter: json_formatter
          filename: '/home/aries/log/acapy-agent.log'
          when: 'd'
          interval: 7
          backupCount: 1
      loggers:
        '':
          level: ERROR
          handlers:
            - stream_handler
            - timed_file_handler
